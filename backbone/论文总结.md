# Transformer



### attention all you need

- 提出注意力机制



### swin transformer 

- 滑动窗口 小窗口自注意力机制（位置编码 矩阵乘法）

- 移动窗口 循环位移 割补注意力块 还原块
- 移动窗口做自注意力复杂度更低
- path merging 合并patch 增加感受野 抓住多尺度信息



### vision tranformer

An image is worth 16x16 words Transformers for image recognition at scale



- 提出把图像送进transformer
- 把图像打成一块块送进去



### Scalablevit

Scalablevit:Rethinking the context-oriented generalization

of vision  transformer

- SSA(可伸缩注意力机制) 改变之前transformer模型固定qkv维度

  - 解除QKV与输入维度的绑定

  - 把qkv的空间维度N和通道维度C放缩到 $N\cdot r_n , C\cdot r_c$





- ISWA（解决swin transformer块间的自注意力问题）



### mobileVit

- Inverted bottleneck block 到残差结构 获得更好的特征
- 用cnn获取的特征做块间自注意力



### Going deeper with Image Transformer

- 先做注意力后做token
- 残差块的输出上添加一个可学习的对角矩阵 让残差加上有用的信息



### CPE

CONDITIONAL POSITIONAL ENCODINGS FOR VISION
TRANSFORMERS



对transformer的token进行相对位置编码

CPE的优势 

- 是动态生成的，并以输入标记的局部邻域为条件

- 可以为更长的序列编码
- 在transformer的encode中整合相对位置编码
- 具有平移等变性(知道邻居是谁)





### Mobile-Former



Mobile-Former: Bridging MobileNet and Transformer



 cnn与transformer并行化，两边相互传输数据

![image-20230502105108442](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230502105108442.png)



![image-20230502105203212](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230502105203212.png)



双向桥的MobileNet

- 图片作为输入



双向桥的Former

- 可学习的参数（token）作为输入 （这些token是随机初始化的）
  - Former相对比与vit有更少的token  每个token代表图像的全局信息





核心： A light-weight cross attention

- 让attention的qkv来自于两个不同的分支









### PVT(Pyramid Vision Transformer)

Pyramid Vision Transformer: A Versatile Backbone for Dense Prediction without Convolutions



优点

- 全局感受野（global receptive field）
- 金字塔结构
- 完全无卷积
- 专为各种像素级密集预测任务设计的纯transformer主干
- 减少参数量(减少特征图尺寸 增加特征通道)



![image-20230502110520356](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230502110520356.png)







### CSWin Transformer

CSWin Transformer: A General Vision Transformer Backbone with Cross-Shaped Windows



- 十字架Transfromer
- 用于并行计算形成十字形窗口的水平和垂直条纹中的 self-attention，每个条纹都是通过将输入特征分成等宽的条纹获得的。





### EdgeNeXt



EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications



传统Transformer不能用于移动设备，计算量大。提出用于移动视觉应用的高效合并CNN-Transformer架构



提出dw卷积融合transformer的结构

- dw卷积可以融合多尺度特征，更大的感受野
- split开然后dw卷积然后进行 skin connection 跳跃连接

![image-20230502102601716](C:/Users/Administrator/AppData/Roaming/Typora/typora-user-images/image-20230502102601716.png)









# ConvNet





### ResNet

deep residual learning for image recognition 

- ResNet 残差网络结构

- $$
  F(x) = H(x) - x
  $$

  $$
  F(x) + x
  $$

  













### a convnet for the 2020s

- 对比convnet和transformer的结构
- 让convnet采用transformer的结构性能比transformer更好













### FCN

- 全卷积网络
- 利用卷积代替linear (可以得到像素的类别)
- 提出skip连接



在卷积网络最后一层的都会进行全连接，如VGG和Resnet，都会在网络的最后加入一些全连接层，经过softmax后就可以获得类别概率信息。但是这个概率信息是1维的，即只能标识整个图片的类别，不能标识每个像素点的类别，所以这种全连接方法不适用于图像分割。



![image-20230502115610203](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230502115610203.png)





![image-20230502115723629](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230502115723629.png)











### FPN



Feature Pyramid Networks for Object Detection





 这篇论文主要解决的问题是目标检测在处理多尺度变化问题是的不足。

小物体在进行卷积操作提取特征时，因为尺寸小在下采样过程中极易被丢失信息。为了处理这种物体大小差异十分明显的检测问题，经典的方法是利用图像金字塔的方式进行多尺度变化增强，但这样会带来极大的计算量。所以这篇论文提出了特征金字塔的网络结构，能在增加极小的计算量的情况下，处理好物体检测中的多尺度变化问题。



首先提取不同尺寸的特征图 1，2，3 构成一个金字塔，然后复制塔顶得到4 处理4 与2进行融合得到5，自上而下融合

![image-20230502162111855](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230502162111855.png)









### MixConv Mixed Depthwise Convolutional Kernels

- 给图像的channel分组 不同组使用不同的卷积核 进行dw卷积

- 对input的c分组 不同的组使用不同的卷积核



![image-20230502104904463](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230502104904463.png)







### ConvNeXt v2

在ConvNeXt中添加自监督学习技术进行预训练

- 提出了一个完全卷积掩码自动编码器框架（fully convolutional masked autoencoder framework）(FCMAE)
  - 传统的MAE基于Vit提出来把patch加噪声弄模糊 在用来预训练 这个预训练方法不能直接用在卷积网络上
  - 使用全卷积掩码自动编码器框架进行了**预训练**(这样可以更好的学习特征)
    - 进行卷积时遮住卷积操作时的部分元素

- 一个新的全局响应归一化 (GRN) 层，可以将其添加到 ConvNeXt 架构中以增强通道间特征竞争。



![image-20230501154720388](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230501154720388.png)







### ParC-Net



ParC-Net: Position Aware Circular Convolution with Merits from ConvNets and Transformer



Transformer很牛 可以做全局注意力，但是它的复杂度和计算量让他无法在小设备上

卷积如何变得和他那么牛呢，让卷积的感受野变大



ParC-Net架构 让卷积的感受野变大

![image-20230428090214763](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230428090214763.png)





input + 位置编码  -> 堆叠 -> dw卷积  两个不同位置连接起来 卷积的感受野大大提高

图中*为dw卷积 卷积核的形状为长条    侧面和水平方向做dw卷积







### MobileNet V1-3



 V1

- 提出dw卷积



V2

- 倒残差结构(Inverted bottleneck block)



v3

- SE通道注意力结构
- 新的激活函数h-swish(x)代替Relu6

![image-20230501163941330](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230501163941330.png)















### DMFORMER 



DMFORMER CLOSING THE GAP BETWEEN CNN AND VISION TRANSFORMERS



网络主要结构

![image-20230502104211492](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230502104211492.png)



- 左边通过多个dw卷积提取不同尺度的特征然后将不同层次的特征进行融合
- 右边有个门控机制启动自适应输入权重(DMA)
  - 通过门控机制学习挑选信息特征
  - GAP获取全局信息 接两个FC最后sigmoid计算注意力向量
  - 这个注意力向量通过与融合特征做矩阵乘法 校准不同特征融合的权重 ，让这些特征更好的融合 融合输出











###              

MULTI-SCALE CONTEXT AGGREGATION BY DILATED CONVOLUTIONS





新的密集预测卷积网络模块 contextnetwork

- 使用扩张卷积
  - 扩张卷积支持以指数方式扩展感受野，而不会损失分辨率或覆盖范围

- 可以在不丢失分辨率或分析重新缩放的图像的情况下聚合多尺度上下文信息



![image-20230502105910836](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230502105910836.png)













# MLP Net



### VIP

- vison permutator
- 一种来自MLP的视觉分类网络
- H，W，C维度空间信息融合



![image-20230501154815828](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230501154815828.png)



![image-20230501154827398](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230501154827398.png)







### CYCLEMLP



CYCLEMLP: A MLP-LIKE ARCHITECTURE FOR DENSE PREDICTION



传统的MLP架构与图像大小相关，因此在对象检测和分割中不行。我们提出了CycleMLP



- 可以应对各种图像尺寸
- 通过使用局部窗口实现了图像尺寸的线性计算复杂度(新的FC结构 复杂度更低)



![image-20230501155048747](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230501155048747.png)



![image-20230501155342654](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230501155342654.png)







### Meta Former



MetaFormer is Actually What You Need for Vision



最近的研究表明T的注意力模块可以被MLP取代，并且生成的模型效果很好，我们弄了一个PoolFormer并且效果很好， 使用MLP结构（PoolingFormer）代替Transformer，让pooling作为 token mixer



提出MLP模型 PoolFormer

- 使用池化来融合特征



![image-20230501162636742](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230501162636742.png)







### ParC-Net

- 把Transformer的优点融合到卷积神经网络里面

- 分别使用垂直方向和水平方向的卷积

  ![image-20230428084729297](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230428084729297.png)



循环卷积ParC

![image-20230428090214763](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230428090214763.png)



ParC有两种类型

- ParC-V垂直方向 如图a 提取垂直方向感受野
- ParC-H水平方向  如图b 提取水平方向感受野
- 水平 垂直方向插入位置信息







# 目标检测
