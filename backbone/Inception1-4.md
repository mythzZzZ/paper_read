# Going Deeper with Convolutions

2015 CVPR  GoogLeNet



这篇论文要解决什么问题？要验证一个什么科学假设？（宏观）

 提高深度神经网络性能的最直接方法是增加其规模

- 宽度
- 深度

但是这两个方法有两个主要的缺点

- 更大的尺寸意味着更多的参数，扩大网络容易过度拟合，特别是在数据集太小的情况下
- 计算资源使用增加（增加卷积 如果增加的卷积使用效率低下，大多数权重最终接近与0，则大部分计算都会被浪费），计算资源的有效分配也优于不加选择地增加大小









论文中提到的解决方案是什么，关键点在哪儿？



为了解决上面的两个问题，提出了稀疏性，构造稀疏神经网络，即使在卷积内部也是如此

- 提出Inception结构，增加网络深度，增加网络宽度，不会造成明显的性能损失

- Hebbian principle – neurons that fire together, wire together ,Inception结构生物学的支撑 赫布原理，神经元一起放电，连接在一起



关于稀疏矩阵计算的大量文献（例如[3]）表明，将稀疏矩阵聚类成相对密集的子矩阵往往会给稀疏矩阵乘法带来有竞争力的性能。



使用Inception 在计算需求会增加太多的地方明智的减少维度

1×1 卷积用于在昂贵的 3×3 和 5×5 卷积之前计算归约。 除了用作缩减之外，它们还包括使用==修正线性激活==，使其具有双重用途



![image-20230715112545653](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230715112545653.png)





# Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift



这篇论文要解决什么问题？要验证一个什么科学假设？（宏观）

训练深度神经网络很复杂，因为在训练过程中，随着前一层参数的变化，每层输入的分布也会发生变化。 这需要较低的学习率和仔细的参数初始化，从而减慢了训练速度，并且使得训练具有饱和非线性的模型变得非常困难。 我们将这种现象称为内部协变量偏移，并通过标准化层输入来解决该问题。







论文中提到的解决方案是什么，关键点在哪儿？



我们的方法的优势在于将归一化作为模型架构的一部分，并对每个训练小批量执行归一化。 批量归一化允许我们使用更高的学习率，并且在初始化时不那么小心，并且在某些情况下消除了 Dropout 的需要







内部协变量偏移

- 我们将内部协变量偏移定义为由于训练期间网络参数的变化而导致的网络激活分布的变化

BN

- 减少内部协变量偏移
- 加快网络训练速度
- 通过修复层输入的均值和方差的标准化步骤来实现这一点
  - 减少梯度对参数规模或其初始值的依赖性
- 批量归一化还对通过网络的梯度流产生有益的影响。 这使我们能够使用更高的学习率，而不会出现发散的风险































# Rethinking the Inception Architecture for Computer Vision





这篇论文要解决什么问题？要验证一个什么科学假设？（宏观）

尽管增加的模型大小和计算成本往往会转化为大多数任务的直接质量增益（只要为训练提供足够的标记数据），但计算效率和低参数数量仍然是各种用例（例如移动视觉和大数据）的促成因素







论文中提到的解决方案是什么，关键点在哪儿？

网络设计原则

- 在达到用于当前任务的最终表示之前，表示大小应从输入到输出逐渐减小（HXW变小）
- 增加卷积网络中每个图块的激活可以实现更多的解缠结特征。 由此产生的网络将训练得更快
- 空间聚合可以在较低维度的嵌入上完成，而不会损失太多或任何表示能力 （在低纬度特征时，3x3conv 前使用1x1conv 降维，由于在低纬度通道间信息的相关性比较大，1x1降维不会丢失很多的信息，通过这样的降维可以加速学习）
- 平衡网络的宽度和深度，平衡的增加网络的深度和宽度可以使效果更好











提出分解大卷积核的策略

- 使用因式分解把大卷积核分解为更小的卷积，（两层3x3效果与5x5差不多）
  - （例如 5 × 5 或 7 × 7）的卷积在计算方面往往会非常昂贵。 例如，在具有 m 个滤波器的网格上使用 n 个滤波器的 5 × 5 卷积的计算成本是具有相同数量滤波器的 3 × 3 卷积的 25/9 = 2.78 倍，5×5 滤波器可以捕获较早层中较远的单元激活之间的信号之间的依赖关系，因此滤波器几何尺寸的减小会导致表达能力的巨大损失
- 空间分解为不对称卷积 1xn  nx1  比第一种方法节省更多的计算量
  - 其中1xn 卷积在中等网格大小上（在 m×m 特征图上，其中 m 范围在 12 到 20 之间）给出了非常好的结果





![image-20230715121805495](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230715121805495.png)



















# Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning



这篇论文要解决什么问题？要验证一个什么科学假设？（宏观）

Inception + ResNet的结合





我们给出了明确的经验证据，表明使用剩余连接进行训练可以显着加速 Inception 网络的训练。 还有一些证据表明，残差 Inception 网络的性能优于同样昂贵的无残差连接的 Inception 网络。











论文中提到的解决方案是什么，关键点在哪儿？



在将残差添加到前一层激活之前缩小残差似乎可以稳定训练。 一般来说，我们在将残差添加到累积层激活之前选择一些介于 0.1 和 0.3 之间的缩放因子来缩放残差





![image-20230719151453646](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230719151453646.png)

