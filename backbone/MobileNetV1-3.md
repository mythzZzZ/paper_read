

==MobileNet==

==v1==

这篇论文要解决什么问题？要验证一个什么科学假设？（宏观）

 人们对构建网络越来越有兴趣 作者自己弄了个网络

作者想弄一个移动端的网络 希望这个网络小 延迟低 速度更快



这篇论文有哪些相关研究，这些研究是怎么分类的？有哪些研究员值得关注？（细节）

**相关研究：**

 

论文中提到的解决方案是什么，关键点在哪儿？

 使用dw卷积来构建轻量级深度神经网络 网络小 延迟低 轻松满足移动和嵌入式视觉应用程序的设计需求

-  使用dw卷积
- 使用宽度倍增器和分辨率倍增器构建更小，更快的MobileNet(牺牲合理的精度来减小尺寸和延迟)
- **MobileNetV1就是把VGG中的标准卷积层换成深度可分离卷积就可以了**



论文中的实验是如何设计的？各个实验分别得到了什么结论？（细节）

 消融实验 研究深度卷积的效果以及减少网络宽度而不是层数来缩小选择

- 使用了dw卷积 参数量下降为原来的1/9到1/8
- 使用ReLu6激活函数 在低精度计算下具有更强的鲁棒性







用于定量评估的数据集是什么？代码开源的话找到链接（细节）

**实验数据集：**

coco ImageNet

 

这篇论文到底有什么贡献？（三句话内说明）新在什么地方？（宏观）

**本文贡献：**

 主要使用了dw卷积 使网络轻量化 参数量更小而且有不错的准确率



下一步还能基于它做什么？有什么工作可以继续深入？（宏观）

其他地方使用dw卷积









==v2==



这篇论文要解决什么问题？要验证一个什么科学假设？（宏观）

通过实验发现 低纬度relu会造成信息的丢失 在高纬度relu信息丢失会很少





这篇论文有哪些相关研究，这些研究是怎么分类的？有哪些研究员值得关注？（细节）

**相关研究：**

 

论文中提到的解决方案是什么，关键点在哪儿？

 低纬度relu会造成信息的丢失 在高纬度relu信息丢失会很少 解决这个问题

- 这个问题导致深度卷积的卷积核有不少是空的

- 把网络最后那一个relu6换成Linear（称为**linear bottleneck**）
- 扩张通道 dw卷积在低维通道效果不好 1x1卷积扩张通道然后dw卷积

- 加入shortcut结构

论文中的实验是如何设计的？各个实验分别得到了什么结论？（细节）

 ![image-20230421165332501](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230421165332501.png)

用于定量评估的数据集是什么？代码开源的话找到链接（细节）

**实验数据集：** ImageNet

 coco





这篇论文到底有什么贡献？（三句话内说明）新在什么地方？（宏观）

**本文贡献：** 发现了dw卷积的特点 作用在低纬度时效果不明显 所以先升高维度然后dw卷积然后降低维度

 

下一步还能基于它做什么？有什么工作可以继续深入？（宏观）









![image-20230421170801219](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230421170801219.png)





==v3==

- SE通道注意力结构
- 新的激活函数h-swish(x)代替Relu6

![image-20230501163941330](https://zhangwenkang666.oss-cn-beijing.aliyuncs.com/image-20230501163941330.png)

